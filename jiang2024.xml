<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2273</fr:anchor><fr:addr
type="user">jiang2024</fr:addr><fr:route>jiang2024.xml</fr:route><fr:title
text="Reinforcement Learning: Theory and Algorithms">Reinforcement Learning: Theory and Algorithms</fr:title><fr:taxon>Reference</fr:taxon><fr:authors><fr:author><fr:link
type="local"
href="nanjiang.xml"
addr="nanjiang"
title="Nan Jiang">Nan Jiang</fr:link></fr:author></fr:authors><fr:meta
name="external">https://rltheorybook.github.io/rltheorybook_AJKS.pdf</fr:meta></fr:frontmatter><fr:mainmatter /><fr:backmatter><fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:title
text="Backlinks">Backlinks</fr:title><fr:authors /></fr:frontmatter><fr:mainmatter><fr:tree
toc="true"
numbered="false"
show-heading="true"
show-metadata="true"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2275</fr:anchor><fr:addr
type="user">kak-000X</fr:addr><fr:route>kak-000X.xml</fr:route><fr:title
text="Notes on Reinforcement Learning: Theory and Algorithms">Notes on <fr:link
type="local"
href="jiang2024.xml"
addr="jiang2024"
title="Reinforcement Learning: Theory and Algorithms">Reinforcement Learning: Theory and Algorithms</fr:link></fr:title><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>To start, I am gonna refresh / strengthen my theoretical understanding through <fr:link
type="local"
href="jiang2024.xml"
addr="jiang2024"
title="Reinforcement Learning: Theory and Algorithms">Reinforcement Learning: Theory and Algorithms</fr:link>.</fr:p><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>806</fr:anchor><fr:addr
type="user">kak-000Y</fr:addr><fr:route>kak-000Y.xml</fr:route><fr:title
text="Markov Decision Processes">Markov Decision Processes</fr:title><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>In reinforcement learning, the interactions between the agent and the environment are often described by an infinite-horizon, discounted Markov Decision Process (MDP).</fr:p><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>807</fr:anchor><fr:addr
type="user">kak-000Z</fr:addr><fr:route>kak-000Z.xml</fr:route><fr:title
text="Markov Decision Process">Markov Decision Process</fr:title><fr:taxon>Definition</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>A Markov Decision Process <fr:tex
display="inline"><![CDATA[M=(S,A,P,\tau ,\gamma ,\mu )]]></fr:tex> is a tuple, where

<fr:ul><fr:li>A state space <fr:tex
display="inline"><![CDATA[S]]></fr:tex>, which may be finite or infinite. For mathematical convenience, we will assume that <fr:tex
display="inline"><![CDATA[S]]></fr:tex> is finite or countably infinite.</fr:li>
  <fr:li>A action space <fr:tex
display="inline"><![CDATA[A]]></fr:tex>, which also may be discrete or infinite. For mathematical convenience, we will assume that <fr:tex
display="inline"><![CDATA[A]]></fr:tex> is finite.</fr:li>
  <fr:li>A transition function <fr:tex
display="inline"><![CDATA[P:S\times  A\to \Delta (S)]]></fr:tex>, where <fr:tex
display="inline"><![CDATA[\Delta (S)]]></fr:tex> is the space of probability distributions over <fr:tex
display="inline"><![CDATA[S]]></fr:tex> (i.e., the below) <fr:tex
display="inline"><![CDATA[P_{S}]]></fr:tex> is <fr:tex
display="inline"><![CDATA[\mathcal {P}(S|\tau _{0})]]></fr:tex> is the probability of transitioning to state <fr:tex
display="inline"><![CDATA[s^{\prime }]]></fr:tex> upon taking action <fr:tex
display="inline"><![CDATA[a]]></fr:tex> in state <fr:tex
display="inline"><![CDATA[s]]></fr:tex>.</fr:li>
  <fr:li>A reward function <fr:tex
display="inline"><![CDATA[r: S\times  A \to  [0,1]]]></fr:tex>. <fr:tex
display="inline"><![CDATA[r(s, a)]]></fr:tex> is the immediate reward associated with taking action <fr:tex
display="inline"><![CDATA[a]]></fr:tex> in state <fr:tex
display="inline"><![CDATA[s]]></fr:tex>. More generally, the <fr:tex
display="inline"><![CDATA[r(s,a)]]></fr:tex> could be a random variable (where the distribution depends on <fr:tex
display="inline"><![CDATA[a]]></fr:tex>, <fr:tex
display="inline"><![CDATA[0]]></fr:tex>). While we largely focus on the case where <fr:tex
display="inline"><![CDATA[r(s,a)]]></fr:tex> is deterministic, the extension to methods with stochastic rewards are often straightforward.</fr:li>
  <fr:li>A discount factor <fr:tex
display="inline"><![CDATA[\gamma  \in  (0,1)]]></fr:tex>, which defines a horizon for the problem.</fr:li></fr:ul></fr:p></fr:mainmatter><fr:backmatter /></fr:tree><fr:p>From an MDP and a policy <fr:tex
display="inline"><![CDATA[\pi  : S \to  A]]></fr:tex>, we can define a Value function for <fr:tex
display="inline"><![CDATA[\pi ]]></fr:tex>.</fr:p><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>808</fr:anchor><fr:addr
type="user">kak-0010</fr:addr><fr:route>kak-0010.xml</fr:route><fr:title
text="Value Function">Value Function</fr:title><fr:taxon>Definition</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>For a fixed policy and a starting state <fr:tex
display="inline"><![CDATA[s_{0}=s]]></fr:tex>, we define the value function <fr:tex
display="inline"><![CDATA[V_{M}^{s}:S\to \mathbb {R}]]></fr:tex> as the discounted sum of future rewards

<fr:tex
display="block"><![CDATA[V_{M}^{s}(s)=\mathbb {E}\Big [\sum _{t=0}^{\infty }\gamma ^{t}r(s_{t},a_{t})\mid \pi ,s_{0}=s\Big ]]]></fr:tex></fr:p></fr:mainmatter><fr:backmatter /></fr:tree><fr:p>The expectation is over the randomness of the the transitions <fr:tex
display="inline"><![CDATA[P]]></fr:tex> and if the policy <fr:tex
display="inline"><![CDATA[\pi ]]></fr:tex> is stochastic. When determining a policy we are more interested with the value of an action in a specific state rather than just the state itself. To get this, we can define a <fr:tex
display="inline"><![CDATA[Q]]></fr:tex>-function in a similar way.</fr:p><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>809</fr:anchor><fr:addr
type="user">kak-0011</fr:addr><fr:route>kak-0011.xml</fr:route><fr:title
text="Action Value Function">Action Value Function</fr:title><fr:taxon>Definition</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>For a fixed policy and a starting state <fr:tex
display="inline"><![CDATA[s_{0}=s]]></fr:tex>, we define the action value function <fr:tex
display="inline"><![CDATA[Q_{M}^{s}:S \times  A \to \mathbb {R}]]></fr:tex> as the discounted sum of future rewards after taking action <fr:tex
display="inline"><![CDATA[A]]></fr:tex>.

<fr:tex
display="block"><![CDATA[Q_{M}^{s}(s, a)=\mathbb {E}\Big [\sum _{t=0}^{\infty }\gamma ^{t}r(s_{t},a_{t})\mid \pi , s_{0}=s, a_{0} = a\Big ]]]></fr:tex></fr:p></fr:mainmatter><fr:backmatter /></fr:tree><fr:p>If the reward <fr:tex
display="inline"><![CDATA[r(s,a)]]></fr:tex> is bounded by some <fr:tex
display="inline"><![CDATA[R_{\text {max}}]]></fr:tex>. Then we can trivially bound both the value and action value function by <fr:tex
display="inline"><![CDATA[(R_{max}/1 - \gamma )]]></fr:tex></fr:p>
   
   <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>810</fr:anchor><fr:addr
type="machine">#305</fr:addr><fr:route>unstable-305.xml</fr:route><fr:taxon>Proof</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
    This is just a geometric series i.e. 
<fr:tex
display="block"><![CDATA[\begin {align*} V_{M}^{s}(s)&=\mathbb {E}\Big [\sum _{t=0}^{\infty }\gamma ^{t}r(s_{t},a_{t})\mid \pi ,s_{0}=s\Big ] \\ &\leq  \mathbb {E}\Big [\sum _{t=0}^{\infty }\gamma ^{t}R_{\text {max}}\mid \pi ,s_{0}=s\Big ] \\ &= R_{\text {max}} \sum _{t=0}^{\infty }\gamma ^{t} \\ &= R_{\text {max}}/ 1 - \gamma      \end {align*}   ]]></fr:tex>
  </fr:mainmatter><fr:backmatter /></fr:tree>
 

</fr:mainmatter><fr:backmatter /></fr:tree></fr:mainmatter><fr:backmatter /></fr:tree><fr:tree
toc="true"
numbered="false"
show-heading="true"
show-metadata="true"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2277</fr:anchor><fr:addr
type="user">log-0007</fr:addr><fr:route>log-0007.xml</fr:route><fr:title
text="11/21/2024">11/21/2024</fr:title><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:meta
name="author">false</fr:meta></fr:frontmatter><fr:mainmatter><fr:p>Not a maximally productive day but good progress was made on (almost?) every project.</fr:p>
  
    <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>1388</fr:anchor><fr:addr
type="machine">#245</fr:addr><fr:route>unstable-245.xml</fr:route><fr:title
text="Daily Summary">Daily Summary</fr:title><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
  <fr:p><fr:strong>Probability Theory:</fr:strong> I am having a surprisingly good time learning this compared to my experience in undergrad. Today, I finished and thorougly understood the extension of a measure to the <fr:tex
display="inline"><![CDATA[\sigma ]]></fr:tex>-algebra induced by the outer measure. I left the uniqueness for the next session. I guess the only outstanding question I had was 
    <fr:ul><fr:li>Why do we need to impose the extra condition that we need the set to satisfy <fr:tex
display="inline"><![CDATA[P^*(A \cap  E) + P^*(A^c \cap  E) = P^*(E)]]></fr:tex> for every <fr:tex
display="inline"><![CDATA[E \subset  \Omega ]]></fr:tex> vs. just taking <fr:tex
display="inline"><![CDATA[E = \Omega ]]></fr:tex>?</fr:li></fr:ul></fr:p>

  <fr:p><fr:strong>Contrastive RL:</fr:strong> Today was big for my understanding along with an interesting new direction.
    <fr:ul><fr:li>I went over both the original <fr:link
type="local"
href="gutmann2012.xml"
addr="gutmann2012"
title="Noise-contrastive Estimation">NCE</fr:link> and <fr:link
type="local"
href="vandenOord2018.xml"
addr="vandenOord2018"
title="Contrastive Predictive Decoding">InfoNCE</fr:link> papers to better understand the non-RL versions.</fr:li>
      <fr:li>Finally have kind of understood, where the <fr:tex
display="inline"><![CDATA[Q]]></fr:tex>-function comes from in <fr:link
type="local"
href="eysenbach2023.xml"
addr="eysenbach2023"
title="Constrastive Learning as Goal Conditioned Reinforcement Learning">Constrastive Learning as Goal Conditioned Reinforcement Learning</fr:link> even though they do not provide proof of the Lemma that claims it.</fr:li>
      <fr:ul><fr:li>In the <fr:link
type="local"
href="gutmann2012.xml"
addr="gutmann2012"
title="Noise-contrastive Estimation">Noise-contrastive Estimation</fr:link> paper, they show that their objective will converge to the distribution that generated the data <fr:tex
display="inline"><![CDATA[p(x)]]></fr:tex>. In <fr:link
type="local"
href="eysenbach2023.xml"
addr="eysenbach2023"
title="Constrastive Learning as Goal Conditioned Reinforcement Learning">Constrastive Learning as Goal Conditioned Reinforcement Learning</fr:link>, they want to learn the discounted state occupancy measure. To make this their <fr:tex
display="inline"><![CDATA[p(x)]]></fr:tex>, 
          <fr:ol><fr:li>they take their "positive" examples as drawn from the state occupancy (turns out to do this is not that hard).</fr:li>
            <fr:li>they just sample their negative examples uniformly.</fr:li></fr:ol>
          <fr:li>Need to look a little more at <fr:tex
display="inline"><![CDATA[Q]]></fr:tex>-function is state discounted probability proof. Why is it not just linearity of expectation / Fubini? Inner series is clearly convergent? Probably related to policy stuff I need to get better at.</fr:li></fr:li></fr:ul>
        <fr:li>An interesting idea that came up is to apply goal-conditioned RL to safe RL. Namely, we can discourage the visitation of unsafe states by considering <fr:tex
display="inline"><![CDATA[Q(s, a, \text {unsafe})]]></fr:tex></fr:li>
        <fr:ul><fr:li>Take <fr:tex
display="inline"><![CDATA[\pi  \in  \arg \max _{\pi } Q(s, \pi (s), g) - \lambda (\delta ) Q(s, \pi (s), \text {obstacle})]]></fr:tex>, or</fr:li>
            <fr:li>take <fr:tex
display="inline"><![CDATA[\pi  \in  \arg \max _{\pi } Q(s, \pi (s), g)]]></fr:tex>, such that <fr:tex
display="inline"><![CDATA[Q(s, \pi (s), \text {obstacle}) < \epsilon ]]></fr:tex>.</fr:li></fr:ul></fr:ul></fr:p>
<fr:p><fr:strong>Ghost in the shell:</fr:strong> New ideas have arisen to turn this course project into a research project.
<fr:ul><fr:li>Professors introduce the concept of "phase detection" from microarchitecture.</fr:li>
    <fr:li>It seems that this area is concerned with detecting how inputs may influence things like hot paths and detecting when this is going to occur.</fr:li>
    <fr:li>These methods all seems to choose metrics and algorithms for these metrics arbitrarily.</fr:li>
    <fr:li>Can we pass a superset of all these metrics to an LLM and have it detect phase changes automatically?</fr:li></fr:ul></fr:p>
</fr:mainmatter><fr:backmatter /></fr:tree>
  

  
    <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>1390</fr:anchor><fr:addr
type="machine">#246</fr:addr><fr:route>unstable-246.xml</fr:route><fr:title
text="Tomorrow Todo's">Tomorrow Todo's</fr:title><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
  <fr:ul><fr:li>I want to start <fr:link
type="local"
href="jiang2024.xml"
addr="jiang2024"
title="Reinforcement Learning: Theory and Algorithms">Reinforcement Learning: Theory and Algorithms</fr:link> tomorrow, taking a short break from very abstract / rigorous mathematics.</fr:li>
    <fr:li>Focus tomorrow is getting profiling up and running.</fr:li>
    <fr:ul><fr:li>By EOD, need to have edges of CFG filled with visitation frequencies.</fr:li></fr:ul>
    <fr:li>If extra time, finish / work on <fr:link
type="local"
href="kak-0005.xml"
addr="kak-0005"
title="Contrastive Reinforcement Learning">Contrastive Reinforcement Learning</fr:link> blog post with newfound knowledge.</fr:li></fr:ul>
</fr:mainmatter><fr:backmatter /></fr:tree>
  
</fr:mainmatter><fr:backmatter /></fr:tree></fr:mainmatter><fr:backmatter /></fr:tree></fr:backmatter></fr:tree>