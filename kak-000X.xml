<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>1249</fr:anchor><fr:addr
type="user">kak-000X</fr:addr><fr:route>kak-000X.xml</fr:route><fr:title
text="Notes on Reinforcement Learning: Theory and Algorithms">Notes on <fr:link
type="local"
href="jiang2024.xml"
addr="jiang2024"
title="Reinforcement Learning: Theory and Algorithms">Reinforcement Learning: Theory and Algorithms</fr:link></fr:title><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>To start, I am gonna refresh / strengthen my theoretical understanding through <fr:link
type="local"
href="jiang2024.xml"
addr="jiang2024"
title="Reinforcement Learning: Theory and Algorithms">Reinforcement Learning: Theory and Algorithms</fr:link>.</fr:p><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>806</fr:anchor><fr:addr
type="user">kak-000Y</fr:addr><fr:route>kak-000Y.xml</fr:route><fr:title
text="Markov Decision Processes">Markov Decision Processes</fr:title><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>In reinforcement learning, the interactions between the agent and the environment are often described by an infinite-horizon, discounted Markov Decision Process (MDP).</fr:p><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>807</fr:anchor><fr:addr
type="user">kak-000Z</fr:addr><fr:route>kak-000Z.xml</fr:route><fr:title
text="Markov Decision Process">Markov Decision Process</fr:title><fr:taxon>Definition</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>A Markov Decision Process <fr:tex
display="inline"><![CDATA[M=(S,A,P,\tau ,\gamma ,\mu )]]></fr:tex> is a tuple, where

<fr:ul><fr:li>A state space <fr:tex
display="inline"><![CDATA[S]]></fr:tex>, which may be finite or infinite. For mathematical convenience, we will assume that <fr:tex
display="inline"><![CDATA[S]]></fr:tex> is finite or countably infinite.</fr:li>
  <fr:li>A action space <fr:tex
display="inline"><![CDATA[A]]></fr:tex>, which also may be discrete or infinite. For mathematical convenience, we will assume that <fr:tex
display="inline"><![CDATA[A]]></fr:tex> is finite.</fr:li>
  <fr:li>A transition function <fr:tex
display="inline"><![CDATA[P:S\times  A\to \Delta (S)]]></fr:tex>, where <fr:tex
display="inline"><![CDATA[\Delta (S)]]></fr:tex> is the space of probability distributions over <fr:tex
display="inline"><![CDATA[S]]></fr:tex> (i.e., the below) <fr:tex
display="inline"><![CDATA[P_{S}]]></fr:tex> is <fr:tex
display="inline"><![CDATA[\mathcal {P}(S|\tau _{0})]]></fr:tex> is the probability of transitioning to state <fr:tex
display="inline"><![CDATA[s^{\prime }]]></fr:tex> upon taking action <fr:tex
display="inline"><![CDATA[a]]></fr:tex> in state <fr:tex
display="inline"><![CDATA[s]]></fr:tex>.</fr:li>
  <fr:li>A reward function <fr:tex
display="inline"><![CDATA[r: S\times  A \to  [0,1]]]></fr:tex>. <fr:tex
display="inline"><![CDATA[r(s, a)]]></fr:tex> is the immediate reward associated with taking action <fr:tex
display="inline"><![CDATA[a]]></fr:tex> in state <fr:tex
display="inline"><![CDATA[s]]></fr:tex>. More generally, the <fr:tex
display="inline"><![CDATA[r(s,a)]]></fr:tex> could be a random variable (where the distribution depends on <fr:tex
display="inline"><![CDATA[a]]></fr:tex>, <fr:tex
display="inline"><![CDATA[0]]></fr:tex>). While we largely focus on the case where <fr:tex
display="inline"><![CDATA[r(s,a)]]></fr:tex> is deterministic, the extension to methods with stochastic rewards are often straightforward.</fr:li>
  <fr:li>A discount factor <fr:tex
display="inline"><![CDATA[\gamma  \in  (0,1)]]></fr:tex>, which defines a horizon for the problem.</fr:li></fr:ul></fr:p></fr:mainmatter><fr:backmatter /></fr:tree><fr:p>From an MDP and a policy <fr:tex
display="inline"><![CDATA[\pi  : S \to  A]]></fr:tex>, we can define a Value function for <fr:tex
display="inline"><![CDATA[\pi ]]></fr:tex>.</fr:p><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>808</fr:anchor><fr:addr
type="user">kak-0010</fr:addr><fr:route>kak-0010.xml</fr:route><fr:title
text="Value Function">Value Function</fr:title><fr:taxon>Definition</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>For a fixed policy and a starting state <fr:tex
display="inline"><![CDATA[s_{0}=s]]></fr:tex>, we define the value function <fr:tex
display="inline"><![CDATA[V_{M}^{s}:S\to \mathbb {R}]]></fr:tex> as the discounted sum of future rewards

<fr:tex
display="block"><![CDATA[V_{M}^{s}(s)=\mathbb {E}\Big [\sum _{t=0}^{\infty }\gamma ^{t}r(s_{t},a_{t})\mid \pi ,s_{0}=s\Big ]]]></fr:tex></fr:p></fr:mainmatter><fr:backmatter /></fr:tree><fr:p>The expectation is over the randomness of the the transitions <fr:tex
display="inline"><![CDATA[P]]></fr:tex> and if the policy <fr:tex
display="inline"><![CDATA[\pi ]]></fr:tex> is stochastic. When determining a policy we are more interested with the value of an action in a specific state rather than just the state itself. To get this, we can define a <fr:tex
display="inline"><![CDATA[Q]]></fr:tex>-function in a similar way.</fr:p><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>809</fr:anchor><fr:addr
type="user">kak-0011</fr:addr><fr:route>kak-0011.xml</fr:route><fr:title
text="Action Value Function">Action Value Function</fr:title><fr:taxon>Definition</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>For a fixed policy and a starting state <fr:tex
display="inline"><![CDATA[s_{0}=s]]></fr:tex>, we define the action value function <fr:tex
display="inline"><![CDATA[Q_{M}^{s}:S \times  A \to \mathbb {R}]]></fr:tex> as the discounted sum of future rewards after taking action <fr:tex
display="inline"><![CDATA[A]]></fr:tex>.

<fr:tex
display="block"><![CDATA[Q_{M}^{s}(s, a)=\mathbb {E}\Big [\sum _{t=0}^{\infty }\gamma ^{t}r(s_{t},a_{t})\mid \pi , s_{0}=s, a_{0} = a\Big ]]]></fr:tex></fr:p></fr:mainmatter><fr:backmatter /></fr:tree><fr:p>If the reward <fr:tex
display="inline"><![CDATA[r(s,a)]]></fr:tex> is bounded by some <fr:tex
display="inline"><![CDATA[R_{\text {max}}]]></fr:tex>. Then we can trivially bound both the value and action value function by <fr:tex
display="inline"><![CDATA[(R_{max}/1 - \gamma )]]></fr:tex></fr:p>
   
   <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>810</fr:anchor><fr:addr
type="machine">#305</fr:addr><fr:route>unstable-305.xml</fr:route><fr:taxon>Proof</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
    This is just a geometric series i.e. 
<fr:tex
display="block"><![CDATA[\begin {align*} V_{M}^{s}(s)&=\mathbb {E}\Big [\sum _{t=0}^{\infty }\gamma ^{t}r(s_{t},a_{t})\mid \pi ,s_{0}=s\Big ] \\ &\leq  \mathbb {E}\Big [\sum _{t=0}^{\infty }\gamma ^{t}R_{\text {max}}\mid \pi ,s_{0}=s\Big ] \\ &= R_{\text {max}} \sum _{t=0}^{\infty }\gamma ^{t} \\ &= R_{\text {max}}/ 1 - \gamma      \end {align*}   ]]></fr:tex>
  </fr:mainmatter><fr:backmatter /></fr:tree>
 

</fr:mainmatter><fr:backmatter /></fr:tree></fr:mainmatter><fr:backmatter><fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:title
text="References">References</fr:title><fr:authors /></fr:frontmatter><fr:mainmatter><fr:tree
toc="true"
numbered="false"
show-heading="true"
show-metadata="true"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>1251</fr:anchor><fr:addr
type="user">jiang2024</fr:addr><fr:route>jiang2024.xml</fr:route><fr:title
text="Reinforcement Learning: Theory and Algorithms">Reinforcement Learning: Theory and Algorithms</fr:title><fr:taxon>Reference</fr:taxon><fr:authors><fr:author><fr:link
type="local"
href="nanjiang.xml"
addr="nanjiang"
title="Nan Jiang">Nan Jiang</fr:link></fr:author></fr:authors><fr:meta
name="external">https://rltheorybook.github.io/rltheorybook_AJKS.pdf</fr:meta></fr:frontmatter><fr:mainmatter /><fr:backmatter /></fr:tree></fr:mainmatter><fr:backmatter /></fr:tree><fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:title
text="Context">Context</fr:title><fr:authors /></fr:frontmatter><fr:mainmatter><fr:tree
toc="true"
numbered="false"
show-heading="true"
show-metadata="true"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>1253</fr:anchor><fr:addr
type="user">kak-0028</fr:addr><fr:route>kak-0028.xml</fr:route><fr:title
text="Reinforcement Learning">Reinforcement Learning</fr:title><fr:date><fr:year>2024</fr:year><fr:month>12</fr:month><fr:day>28</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>902</fr:anchor><fr:addr
type="user">kak-0012</fr:addr><fr:route>kak-0012.xml</fr:route><fr:title
text="Notes on Reinforcement Learning: An Introduction">Notes on <fr:link
type="local"
href="sutton2022.xml"
addr="sutton2022"
title="Reinforcement Learning: An Introduction">Reinforcement Learning: An Introduction</fr:link></fr:title><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>This won't really be notes, I will mainly just store my thoughts and questions about each chapter along with solutions to selected exercises. Specifically, I will be learning the <fr:link
type="local"
href="richardsutton.xml"
addr="richardsutton"
title="Richard Sutton">Richard Sutton</fr:link> flavor of RL through his <fr:link
type="external"
href="https://drive.google.com/drive/folders/0B3w765rOKuKANmxNbXdwaE1YU1k?resourcekey=0-JZz-noRuJgogNsg1ljgV8w">CMPUT 609</fr:link> course. Due to this, I will try to follow his <fr:link
type="external"
href="http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/RLAIcourse/researchdiary.html">recommended style of engagement</fr:link>.</fr:p>
  
  

  
    <fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>787</fr:anchor><fr:addr
type="machine">#304</fr:addr><fr:route>unstable-304.xml</fr:route><fr:title
text="Review Question">Review Question</fr:title><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
  
    <fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>789</fr:anchor><fr:addr
type="machine">#302</fr:addr><fr:route>unstable-302.xml</fr:route><fr:taxon>Problem</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>Review the methods discussed in the first part of the book. What are their strengths and weaknesses? When should they be used?</fr:mainmatter><fr:backmatter /></fr:tree>

  <html:span
xmlns:html="http://www.w3.org/1999/xhtml"
style="white-space: nowrap">
    
 
   
   <fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>791</fr:anchor><fr:addr
type="machine">#303</fr:addr><fr:route>unstable-303.xml</fr:route><fr:taxon>Solution</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
      
  <html:table
xmlns:html="http://www.w3.org/1999/xhtml">
        
  <html:tr>
          
  <html:td />

          
  <html:td><fr:strong>Strength</fr:strong></html:td>

          
  <html:td><fr:strong>Weakness</fr:strong></html:td>

          
  <html:td><fr:strong>Use case</fr:strong></html:td>

        </html:tr>

        
  <html:tr>
          
  <html:td><fr:strong>Bandits</fr:strong></html:td>

          
  <html:td>Simplest case. Can isolate exploration problem.</html:td>

          
  <html:td>No credit assignment. Action only effect next timestep.</html:td>

          
  <html:td>Clinical trials. Things without temporally extended outcomes.</html:td>

        </html:tr>

        
  <html:tr>
          
  <html:td><fr:strong>Dynamic Programming</fr:strong></html:td>

          
  <html:td>Exact solution.</html:td>

          
  <html:td>Complexity blows up with state and action space. Model-based.</html:td>

          
  <html:td>Tabular MDPs.</html:td>

        </html:tr>

        
  <html:tr>
          
  <html:td><fr:strong>Monte Carlo</fr:strong></html:td>

          
  <html:td>Unbiased estimator. Model-free.</html:td>

          
  <html:td>Extremely high variance. Must wait till end of episode.</html:td>

          
  <html:td>When interacting with environment is inexpensive, episode length is short.</html:td>

        </html:tr>

        
  <html:tr>
          
  <html:td><fr:strong>TD methods</fr:strong></html:td>

          
  <html:td>Model-free. Online.</html:td>

          
  <html:td>Biased estimator.</html:td>

          
  <html:td>Environment interaction is cheap but also episode length is long or continuing.</html:td>

        </html:tr>

        
  <html:tr>
          
  <html:td><fr:strong>N step</fr:strong></html:td>

          
  <html:td>Balance bias-variance tradeoff.</html:td>

          
  <html:td>Must choose <fr:tex
display="inline"><![CDATA[n]]></fr:tex>.</html:td>

          
  <html:td>Lower variance continuing environments?</html:td>

        </html:tr>

        
  <html:tr>
          
  <html:td><fr:strong>Dyna</fr:strong></html:td>

          
  <html:td>Sample efficiency.</html:td>

          
  <html:td>Model-based.</html:td>

          
  <html:td>When interacting with environment is costly i.e. driving.</html:td>

        </html:tr>

      </html:table>

    </fr:mainmatter><fr:backmatter /></fr:tree>
 

  </html:span>
</fr:mainmatter><fr:backmatter /></fr:tree>
  

<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>793</fr:anchor><fr:addr
type="user">kak-000G</fr:addr><fr:route>kak-000G.xml</fr:route><fr:title
text="On-policy Prediction with Approximation">On-policy Prediction with Approximation</fr:title><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>5</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>In my opinion, the most important part of this chapter is now that when we update say a <fr:tex
display="inline"><![CDATA[Q]]></fr:tex> function for a specific state action pair <fr:tex
display="inline"><![CDATA[(s,a)]]></fr:tex>, then this update can affect the value of the <fr:tex
display="inline"><![CDATA[Q(s', a')]]></fr:tex> due to the reuse of the internal parameters.</fr:p>
  
    
    
    <fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>794</fr:anchor><fr:addr
type="machine">#306</fr:addr><fr:route>unstable-306.xml</fr:route><fr:title
text=" [sutton2022, 9.1]"> <html:span
xmlns:html="http://www.w3.org/1999/xhtml"
class="link-reference"
tid="9.1"
refid="sutton2022"><fr:link
type="local"
href="sutton2022.xml"
addr="sutton2022"
title="Reinforcement Learning: An Introduction">[sutton2022, 9.1]</fr:link></html:span></fr:title><fr:taxon>Exercise</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>5</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
  
  <fr:p>Show that tabular methods are a special case of linear function approximation. What would the feature vectors be?</fr:p>

</fr:mainmatter><fr:backmatter /></fr:tree>
  

 
   
   <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>795</fr:anchor><fr:addr
type="machine">#307</fr:addr><fr:route>unstable-307.xml</fr:route><fr:taxon>Solution</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>5</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
<fr:p>As a linear function approximation method, we have two quantities <fr:tex
display="inline"><![CDATA[\boldsymbol {w}]]></fr:tex> and <fr:tex
display="inline"><![CDATA[\boldsymbol {x}(s)]]></fr:tex>. We make the update
<fr:tex
display="block"><![CDATA[w_{t + 1} = w_{t} + \alpha (r_t + \gamma  w_{t}^{\top } x_{t+1} - w_t^{\top } x_t) x_t]]></fr:tex>
In the tabular setting, we just assume that we have the capacity to represent every possible state. This means that <fr:tex
display="inline"><![CDATA[x(s) \in  \mathbb {R}^{|S|}]]></fr:tex>. Therefore, if we define the features as <fr:tex
display="inline"><![CDATA[x : s_i \mapsto  \boldsymbol {e}_i]]></fr:tex> then we can recover policy evaluation by taking,
<fr:tex
display="block"><![CDATA[r_i = \mathbb {E}_{a \sim  \pi , s \sim  p}[r(s_i)], \quad  w_i = v_{\pi }(s_i)]]></fr:tex></fr:p>
Substituting, we get 
<fr:tex
display="block"><![CDATA[ w_{t + 1} = w_{t} + \alpha (\mathbb {E}_{a \sim  \pi , s \sim  p}[r(s_i)] + v(s_{t + 1}) - v(s_{t})) \cdot  \boldsymbol {e}_i ]]></fr:tex>
Since <fr:tex
display="inline"><![CDATA[\boldsymbol {w}]]></fr:tex> is our vector of values, updating the <fr:tex
display="inline"><![CDATA[i]]></fr:tex>th entry is exactly performing exactly one update to <fr:tex
display="inline"><![CDATA[v_{\pi }(s_i)]]></fr:tex> in the tabular setting.
</fr:mainmatter><fr:backmatter /></fr:tree>
 

  
    
    
    <fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>796</fr:anchor><fr:addr
type="machine">#308</fr:addr><fr:route>unstable-308.xml</fr:route><fr:title
text=" [sutton2022, 9.2]"> <html:span
xmlns:html="http://www.w3.org/1999/xhtml"
class="link-reference"
tid="9.2"
refid="sutton2022"><fr:link
type="local"
href="sutton2022.xml"
addr="sutton2022"
title="Reinforcement Learning: An Introduction">[sutton2022, 9.2]</fr:link></html:span></fr:title><fr:taxon>Exercise</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>5</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
  
  <fr:p>Why does (9.17) define <fr:tex
display="inline"><![CDATA[(n + 1)^k]]></fr:tex> distinct features for dimension k?</fr:p>

</fr:mainmatter><fr:backmatter /></fr:tree>
  

 
   
   <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>797</fr:anchor><fr:addr
type="machine">#309</fr:addr><fr:route>unstable-309.xml</fr:route><fr:taxon>Solution</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>5</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
  <fr:p>For each <fr:tex
display="inline"><![CDATA[s_j]]></fr:tex>, there are <fr:tex
display="inline"><![CDATA[n + 1]]></fr:tex> options for <fr:tex
display="inline"><![CDATA[c_{i, j}]]></fr:tex>. Since there are <fr:tex
display="inline"><![CDATA[k]]></fr:tex>, <fr:tex
display="inline"><![CDATA[s_{j}]]></fr:tex>'s, there are <fr:tex
display="inline"><![CDATA[(n + 1)^k]]></fr:tex> total possible features for <fr:tex
display="inline"><![CDATA[x_i]]></fr:tex>.</fr:p>
</fr:mainmatter><fr:backmatter /></fr:tree>
 

  
    
    
    <fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>798</fr:anchor><fr:addr
type="machine">#310</fr:addr><fr:route>unstable-310.xml</fr:route><fr:title
text=" [sutton2022, 9.3]"> <html:span
xmlns:html="http://www.w3.org/1999/xhtml"
class="link-reference"
tid="9.3"
refid="sutton2022"><fr:link
type="local"
href="sutton2022.xml"
addr="sutton2022"
title="Reinforcement Learning: An Introduction">[sutton2022, 9.3]</fr:link></html:span></fr:title><fr:taxon>Exercise</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>5</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
  
<fr:p>What <fr:tex
display="inline"><![CDATA[n]]></fr:tex> and <fr:tex
display="inline"><![CDATA[c_{i,j}]]></fr:tex> produce the feature vectors <fr:tex
display="block"><![CDATA[\mathbf {x}(s)=(1,s_{1},s_{2},s_{1}s_{2},s_{1}^{2},s_{2}^{2},s_{1}^{2}s_{2}^{2},s_{1}s_{2}^{2},s_{1}^{2}s_{2}^{2})^{\top }?]]></fr:tex></fr:p>

</fr:mainmatter><fr:backmatter /></fr:tree>
  

 
   
   <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>799</fr:anchor><fr:addr
type="machine">#311</fr:addr><fr:route>unstable-311.xml</fr:route><fr:taxon>Solution</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>5</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
  <fr:p><fr:tex
display="inline"><![CDATA[n = 2]]></fr:tex> and <fr:tex
display="inline"><![CDATA[c_{ij}]]></fr:tex> as 
<fr:tex
display="block"><![CDATA[\boldsymbol {c_0} = [0, 0], \quad    \boldsymbol {c_1} = [1, 0] \\   \boldsymbol {c_2} = [0, 1], \quad    \boldsymbol {c_3} = [1, 1] \\   \boldsymbol {c_4} = [2, 0], \quad    \boldsymbol {c_5} = [0, 2] \\   \boldsymbol {c_6} = [2, 2], \quad    \boldsymbol {c_7} = [1, 2], \quad    \boldsymbol {c_8} = [2, 2]   ]]></fr:tex></fr:p>
</fr:mainmatter><fr:backmatter /></fr:tree>
 

    <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>800</fr:anchor><fr:addr
type="machine">#312</fr:addr><fr:route>unstable-312.xml</fr:route><fr:taxon>Question</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>5</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
  <fr:ol><fr:li>In section 9.5.2, what do they mean when they say you can select <fr:tex
display="inline"><![CDATA[n]]></fr:tex> so all the fourier features can be used?
    <fr:ol><fr:li>Pick <fr:tex
display="inline"><![CDATA[n]]></fr:tex> so that <fr:tex
display="inline"><![CDATA[(n + 1)^k < mk^2]]></fr:tex>.</fr:li>
        <fr:li>Pick <fr:tex
display="inline"><![CDATA[n]]></fr:tex> so that <fr:tex
display="inline"><![CDATA[(n + 1)^k]]></fr:tex> is reasonable.</fr:li></fr:ol></fr:li>
    <fr:p>In the tabular case, I think (a) is correct. My initial understanding of state representation is as a representation learning i.e. compression type objective. If we assume that <fr:tex
display="inline"><![CDATA[s_i \in  [m]]]></fr:tex>, then we do not gain anything in the tabular setting if our value function vector is the same size as the underlying transition kernel. When in the continuous state space regime i.e. <fr:tex
display="inline"><![CDATA[s_i \in  [0,1]]]></fr:tex>, there is no amount of features that would overfit the transition kernel. Therefore, it is just about trying to learn as much as possible about the underlying relations of the state dimensions.</fr:p>
    <fr:p>In the <fr:tex
display="inline"><![CDATA[2]]></fr:tex>-dimensional case, the feature vector <fr:tex
display="inline"><![CDATA[c_i = [1,1]]]></fr:tex> would provide information on how <fr:tex
display="inline"><![CDATA[s_1]]></fr:tex> and <fr:tex
display="inline"><![CDATA[s_2]]></fr:tex> relate not just their specific values. You want the learning algorithm to understand the relationships, so that it can make better use of the internal parameters when learning something like a <fr:tex
display="inline"><![CDATA[Q]]></fr:tex> function. This is basically just SVD but instead of classification you are learning a <fr:tex
display="inline"><![CDATA[Q]]></fr:tex> function. The compression</fr:p></fr:ol>
  </fr:mainmatter><fr:backmatter /></fr:tree>

    <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>801</fr:anchor><fr:addr
type="machine">#313</fr:addr><fr:route>unstable-313.xml</fr:route><fr:taxon>Question</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>5</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
  <fr:ol><fr:li>In Figure 9.5, why do fourier features outperform polynomial features?
    <fr:ol><fr:li>The fourier features got lucky on the seeds (lol).</fr:li>
        <fr:li>The choice of <fr:tex
display="inline"><![CDATA[\boldsymbol {c}]]></fr:tex> is not specified. A good choice can provide improvement specific to the problem?</fr:li>
        <fr:li>Polynomial features range is very large. Can suffer from blowup or vanishing of features.</fr:li></fr:ol></fr:li>
    One of the advantages of fourier features mentioned previously is the ability to select which features to serve as your basis. However, for this setup I assume they just use all of the fourier features. This likely means it is more of an issue with the polynomial features and (c). If you have a large polynomial then even relative similar states <fr:tex
display="inline"><![CDATA[s_1 = 1.1]]></fr:tex>, <fr:tex
display="inline"><![CDATA[s_2 = 0.9]]></fr:tex>, <fr:tex
display="inline"><![CDATA[\ldots ]]></fr:tex> can blow up or vanish making them likely more unstable when using gradient-based methods.</fr:ol>
  </fr:mainmatter><fr:backmatter /></fr:tree>

  
    
    
    <fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>802</fr:anchor><fr:addr
type="machine">#314</fr:addr><fr:route>unstable-314.xml</fr:route><fr:title
text=" [sutton2022, 9.4]"> <html:span
xmlns:html="http://www.w3.org/1999/xhtml"
class="link-reference"
tid="9.4"
refid="sutton2022"><fr:link
type="local"
href="sutton2022.xml"
addr="sutton2022"
title="Reinforcement Learning: An Introduction">[sutton2022, 9.4]</fr:link></html:span></fr:title><fr:taxon>Exercise</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>5</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
  
  <fr:p>You could do anisotropic (big word for asymmetric) tile partitioning. If we consider the two state dimensions as (x,y) coordinates and suppose that we want to only generalize across the <fr:tex
display="inline"><![CDATA[y]]></fr:tex>-direction i.e. we want states with the same <fr:tex
display="inline"><![CDATA[x]]></fr:tex> coordinate to have similar values then we would tile with long thin tiles. Therefore, states with the same <fr:tex
display="inline"><![CDATA[x]]></fr:tex> coordinate would lie in the same vertical tile and if the tiles are very thin any change in <fr:tex
display="inline"><![CDATA[x]]></fr:tex>-coordinate would lie in disjoint tiles.</fr:p>

</fr:mainmatter><fr:backmatter /></fr:tree>
  

    <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>803</fr:anchor><fr:addr
type="machine">#315</fr:addr><fr:route>unstable-315.xml</fr:route><fr:taxon>Remark</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>5</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>I think that RBF as a continuous generalization coarse-coding is a nice intuition I want to remember here. Essentially, you just weight a state by how close it is to the center of the receptive field. This weighting is done via a Gaussian kernel, which I believe is arbitrary and can be any distance measure of choice.</fr:mainmatter><fr:backmatter /></fr:tree>
</fr:mainmatter><fr:backmatter /></fr:tree></fr:mainmatter><fr:backmatter /></fr:tree><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>904</fr:anchor><fr:addr
type="user">kak-000X</fr:addr><fr:route>kak-000X.xml</fr:route><fr:title
text="Notes on Reinforcement Learning: Theory and Algorithms">Notes on <fr:link
type="local"
href="jiang2024.xml"
addr="jiang2024"
title="Reinforcement Learning: Theory and Algorithms">Reinforcement Learning: Theory and Algorithms</fr:link></fr:title><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>To start, I am gonna refresh / strengthen my theoretical understanding through <fr:link
type="local"
href="jiang2024.xml"
addr="jiang2024"
title="Reinforcement Learning: Theory and Algorithms">Reinforcement Learning: Theory and Algorithms</fr:link>.</fr:p><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>806</fr:anchor><fr:addr
type="user">kak-000Y</fr:addr><fr:route>kak-000Y.xml</fr:route><fr:title
text="Markov Decision Processes">Markov Decision Processes</fr:title><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>In reinforcement learning, the interactions between the agent and the environment are often described by an infinite-horizon, discounted Markov Decision Process (MDP).</fr:p><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>807</fr:anchor><fr:addr
type="user">kak-000Z</fr:addr><fr:route>kak-000Z.xml</fr:route><fr:title
text="Markov Decision Process">Markov Decision Process</fr:title><fr:taxon>Definition</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>A Markov Decision Process <fr:tex
display="inline"><![CDATA[M=(S,A,P,\tau ,\gamma ,\mu )]]></fr:tex> is a tuple, where

<fr:ul><fr:li>A state space <fr:tex
display="inline"><![CDATA[S]]></fr:tex>, which may be finite or infinite. For mathematical convenience, we will assume that <fr:tex
display="inline"><![CDATA[S]]></fr:tex> is finite or countably infinite.</fr:li>
  <fr:li>A action space <fr:tex
display="inline"><![CDATA[A]]></fr:tex>, which also may be discrete or infinite. For mathematical convenience, we will assume that <fr:tex
display="inline"><![CDATA[A]]></fr:tex> is finite.</fr:li>
  <fr:li>A transition function <fr:tex
display="inline"><![CDATA[P:S\times  A\to \Delta (S)]]></fr:tex>, where <fr:tex
display="inline"><![CDATA[\Delta (S)]]></fr:tex> is the space of probability distributions over <fr:tex
display="inline"><![CDATA[S]]></fr:tex> (i.e., the below) <fr:tex
display="inline"><![CDATA[P_{S}]]></fr:tex> is <fr:tex
display="inline"><![CDATA[\mathcal {P}(S|\tau _{0})]]></fr:tex> is the probability of transitioning to state <fr:tex
display="inline"><![CDATA[s^{\prime }]]></fr:tex> upon taking action <fr:tex
display="inline"><![CDATA[a]]></fr:tex> in state <fr:tex
display="inline"><![CDATA[s]]></fr:tex>.</fr:li>
  <fr:li>A reward function <fr:tex
display="inline"><![CDATA[r: S\times  A \to  [0,1]]]></fr:tex>. <fr:tex
display="inline"><![CDATA[r(s, a)]]></fr:tex> is the immediate reward associated with taking action <fr:tex
display="inline"><![CDATA[a]]></fr:tex> in state <fr:tex
display="inline"><![CDATA[s]]></fr:tex>. More generally, the <fr:tex
display="inline"><![CDATA[r(s,a)]]></fr:tex> could be a random variable (where the distribution depends on <fr:tex
display="inline"><![CDATA[a]]></fr:tex>, <fr:tex
display="inline"><![CDATA[0]]></fr:tex>). While we largely focus on the case where <fr:tex
display="inline"><![CDATA[r(s,a)]]></fr:tex> is deterministic, the extension to methods with stochastic rewards are often straightforward.</fr:li>
  <fr:li>A discount factor <fr:tex
display="inline"><![CDATA[\gamma  \in  (0,1)]]></fr:tex>, which defines a horizon for the problem.</fr:li></fr:ul></fr:p></fr:mainmatter><fr:backmatter /></fr:tree><fr:p>From an MDP and a policy <fr:tex
display="inline"><![CDATA[\pi  : S \to  A]]></fr:tex>, we can define a Value function for <fr:tex
display="inline"><![CDATA[\pi ]]></fr:tex>.</fr:p><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>808</fr:anchor><fr:addr
type="user">kak-0010</fr:addr><fr:route>kak-0010.xml</fr:route><fr:title
text="Value Function">Value Function</fr:title><fr:taxon>Definition</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>For a fixed policy and a starting state <fr:tex
display="inline"><![CDATA[s_{0}=s]]></fr:tex>, we define the value function <fr:tex
display="inline"><![CDATA[V_{M}^{s}:S\to \mathbb {R}]]></fr:tex> as the discounted sum of future rewards

<fr:tex
display="block"><![CDATA[V_{M}^{s}(s)=\mathbb {E}\Big [\sum _{t=0}^{\infty }\gamma ^{t}r(s_{t},a_{t})\mid \pi ,s_{0}=s\Big ]]]></fr:tex></fr:p></fr:mainmatter><fr:backmatter /></fr:tree><fr:p>The expectation is over the randomness of the the transitions <fr:tex
display="inline"><![CDATA[P]]></fr:tex> and if the policy <fr:tex
display="inline"><![CDATA[\pi ]]></fr:tex> is stochastic. When determining a policy we are more interested with the value of an action in a specific state rather than just the state itself. To get this, we can define a <fr:tex
display="inline"><![CDATA[Q]]></fr:tex>-function in a similar way.</fr:p><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>809</fr:anchor><fr:addr
type="user">kak-0011</fr:addr><fr:route>kak-0011.xml</fr:route><fr:title
text="Action Value Function">Action Value Function</fr:title><fr:taxon>Definition</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>For a fixed policy and a starting state <fr:tex
display="inline"><![CDATA[s_{0}=s]]></fr:tex>, we define the action value function <fr:tex
display="inline"><![CDATA[Q_{M}^{s}:S \times  A \to \mathbb {R}]]></fr:tex> as the discounted sum of future rewards after taking action <fr:tex
display="inline"><![CDATA[A]]></fr:tex>.

<fr:tex
display="block"><![CDATA[Q_{M}^{s}(s, a)=\mathbb {E}\Big [\sum _{t=0}^{\infty }\gamma ^{t}r(s_{t},a_{t})\mid \pi , s_{0}=s, a_{0} = a\Big ]]]></fr:tex></fr:p></fr:mainmatter><fr:backmatter /></fr:tree><fr:p>If the reward <fr:tex
display="inline"><![CDATA[r(s,a)]]></fr:tex> is bounded by some <fr:tex
display="inline"><![CDATA[R_{\text {max}}]]></fr:tex>. Then we can trivially bound both the value and action value function by <fr:tex
display="inline"><![CDATA[(R_{max}/1 - \gamma )]]></fr:tex></fr:p>
   
   <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>810</fr:anchor><fr:addr
type="machine">#305</fr:addr><fr:route>unstable-305.xml</fr:route><fr:taxon>Proof</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
    This is just a geometric series i.e. 
<fr:tex
display="block"><![CDATA[\begin {align*} V_{M}^{s}(s)&=\mathbb {E}\Big [\sum _{t=0}^{\infty }\gamma ^{t}r(s_{t},a_{t})\mid \pi ,s_{0}=s\Big ] \\ &\leq  \mathbb {E}\Big [\sum _{t=0}^{\infty }\gamma ^{t}R_{\text {max}}\mid \pi ,s_{0}=s\Big ] \\ &= R_{\text {max}} \sum _{t=0}^{\infty }\gamma ^{t} \\ &= R_{\text {max}}/ 1 - \gamma      \end {align*}   ]]></fr:tex>
  </fr:mainmatter><fr:backmatter /></fr:tree>
 

</fr:mainmatter><fr:backmatter /></fr:tree></fr:mainmatter><fr:backmatter /></fr:tree><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>906</fr:anchor><fr:addr
type="user">kak-002T</fr:addr><fr:route>kak-002T.xml</fr:route><fr:title
text="Soft Actor Critic">Soft Actor Critic</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>19</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>Want to solve MaxEnt problem subject to constraint on <fr:tex
display="inline"><![CDATA[Q]]></fr:tex> function <fr:tex
display="inline"><![CDATA[\implies ]]></fr:tex> Boltzmann distribution with energy <fr:tex
display="inline"><![CDATA[(Q)]]></fr:tex> function..</fr:p></fr:mainmatter><fr:backmatter /></fr:tree><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>907</fr:anchor><fr:addr
type="user">kak-002U</fr:addr><fr:route>kak-002U.xml</fr:route><fr:title
text="Unreasonable Effectiveness of Eligibility Traces">Unreasonable Effectiveness of Eligibility Traces</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>19</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>keep track of stuff.</fr:p></fr:mainmatter><fr:backmatter /></fr:tree></fr:mainmatter><fr:backmatter /></fr:tree></fr:mainmatter><fr:backmatter /></fr:tree></fr:backmatter></fr:tree>